{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uskmbv/HWPanda_Uskembayeva/blob/main/UskembayevaAltyn_Bonus_Exercise1_MCMLR_2023W.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bonus Exercises 1: Multilingual and Crosslingual Methods and Language Resources**\n",
        "\n",
        "This notebook represents two bonus exercises for the lecture Multilingual and Crosslingual Methods and Language Resources (2023W 340168-1). For each of these you can obtain a maximum of 3 points that are added to the points of your final exam. The sections where your code should go are marked with ðŸ‘‹ âš’.\n",
        "\n",
        "Bonus Exercise 1: Information Extraction with TF-IDF"
      ],
      "metadata": {
        "id": "Uem6oQr40aBL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Information Extraction with TF-IDF**\n",
        "\n",
        "For high-resource languages, supervised approaches represent a viable solution. However, for low-resource languages it is at times necessary to use unsupervised approaches to obtain information from texts. We will work on English here for the sake of understanding the results, but the methods can be applied to any language.\n",
        "\n",
        "For this exercise, you will implement a mini-example of information extraction, or rather feature extraction, with Term Frequency-Inverse Document Frequency (TF-IDF)."
      ],
      "metadata": {
        "id": "54vas_gz00f_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Term Frequency-Inverse Document Frequency (TF-IDF)**\n",
        "\n",
        "In this exercise, you will write a simple implementation of the TF-IDF algorithm and compare your implementation with the one in sklearn. TF-IDF represents an effective method for extracting features from text without any supervision. In documents, there are usually some terms that occur frequently, but might not represent the best features for identifying categories or topics in a document. Instead, TF-IDF assigns higher values to words that occur frequently in one document, but not in all documents. Thereby, they provide more and better information on the potential contents of a document than rare frequency counts. Originally, the technique was used for ranking documents in search engines. Today, it is still used for topic modeling, i.e., identifying topics of documents automatically, term extraction, etc."
      ],
      "metadata": {
        "id": "IrjFknrD6xci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Toy Example of Documents\n",
        "\n",
        "We will use the following toy example, where each sentence in the list is considered a document on its own."
      ],
      "metadata": {
        "id": "CF-Ijn7B99gv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [\"this is the story behind the red house on the street with twenty houses\",\n",
        "      \"a man decided to build his own house on our street, which should change the street\",\n",
        "       \"all the houses were painted white and all the neighbors were happy with this\",\n",
        "       \"the man thought to himself this is wrong and painted his house red\",\n",
        "        \"he went from his house to his neigbor's house and explained\",\n",
        "       \"his neighbor thought this is a good idea and painted his house orange\",\n",
        "       \"now they thought the houses started to look right for a street named rainbow road\"\n",
        "       ]"
      ],
      "metadata": {
        "id": "_GXb_nE8-NCl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use [spaCy](https://spacy.io/) to preprocess these \"documents\" in the list `docs`. The folowing preprocessing steps need to be performed:\n",
        "\n",
        "\n",
        "1.   Tokenization\n",
        "2.   POS tagging\n",
        "3.   Lemmatization\n",
        "\n",
        "We first need to import spaCy and load the English model.\n"
      ],
      "metadata": {
        "id": "sc4YjXLlA8-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "ZdopIdKoCXqc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘‹ âš’  Perform the spacy preprocessing steps described above and remove the POS tags that are indicated in the list below."
      ],
      "metadata": {
        "id": "yr1jvVCzCgfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pos_to_be_removed = ['ADV', 'PRON', 'CCONJ', 'PUNCT', 'PART', 'DET', 'ADP', 'SPACE']\n",
        "\n",
        "\n",
        "def preprocess(sentence):\n",
        "    #tokenization\n",
        "    tokens = nlp(sentence)\n",
        "\n",
        "    #filter out unwanted POS tags\n",
        "    filtered_tokens = [token.lemma_ for token in tokens if token.pos_ not in pos_to_be_removed]\n",
        "\n",
        "    #join the filtered tokens to form a preprocessed sentence\n",
        "    preprocessed_sentence = ' '.join(filtered_tokens)\n",
        "\n",
        "    return preprocessed_sentence\n",
        "\n",
        "#list\n",
        "docs = [\n",
        "    \"this is the story behind the red house on the street with twenty houses\",\n",
        "    \"a man decided to build his own house on our street, which should change the street\",\n",
        "    \"all the houses were painted white and all the neighbors were happy with this\",\n",
        "    \"the man thought to himself this is wrong and painted his house red\",\n",
        "    \"he went from his house to his neighbor's house and explained\",\n",
        "    \"his neighbor thought this is a good idea and painted his house orange\",\n",
        "    \"now they thought the houses started to look right for a street named rainbow road\"\n",
        "]\n",
        "\n",
        "#preprocess\n",
        "preprocessed = []\n",
        "for sentence in docs:\n",
        "    preprocessed.append(preprocess(sentence))\n",
        "\n",
        "\n",
        "print(\"Preprocessed sentences: \", preprocessed)\n"
      ],
      "metadata": {
        "id": "ilDnP6k3B89S",
        "outputId": "2cf5ee3b-0191-4b2e-8ff0-596d4815c6b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed sentences:  ['be story red house street twenty house', 'man decide build own house street should change street', 'house be paint white neighbor be happy', 'man think be wrong paint house red', 'go house neighbor house explain', 'neighbor think be good idea paint house orange', 'think house start look right street name rainbow road']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Term Frequency (TF)\n",
        "\n",
        "The term frequency is calculated as the relative frequency of the word in a specific document, that is, the absolute frequency divided the number of words in the document. For the term $t$ in document  $d$, this is the count of the term $n_{t,d}$ divided by the count of all words $\\sum n_{t',d}$ in the document $d$ :\n",
        "\n",
        "$TF_{i,j} = \\frac{n_{t,d}}{\\sum n_{t',d}}$\n",
        "\n",
        "\n",
        "ðŸ‘‹ âš’  Write a function to calculate the Term Freqquency (TF) for each word in each document. The result will be a list of term frequencies for each document."
      ],
      "metadata": {
        "id": "hpjVDfjf91o-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_term_frequency(bag_of_words):\n",
        "    #count the frequency\n",
        "    term_frequency = {term: bag_of_words.count(term) / len(bag_of_words.split()) for term in set(bag_of_words.split())}\n",
        "\n",
        "    return term_frequency\n",
        "\n",
        "term_frequencies = []\n",
        "for doc in preprocessed:\n",
        "    term_frequencies.append(compute_term_frequency(doc))\n",
        "\n",
        "# Print the computed term frequencies for each document\n",
        "for i, doc_tf in enumerate(term_frequencies, 1):\n",
        "    print(f\"Document {i} Term Frequencies: {doc_tf}\")\n"
      ],
      "metadata": {
        "id": "BCv-y_W_GCYn",
        "outputId": "950f35c7-8b51-41a2-efc8-9610b61695e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1 Term Frequencies: {'red': 0.14285714285714285, 'be': 0.14285714285714285, 'house': 0.2857142857142857, 'twenty': 0.14285714285714285, 'street': 0.14285714285714285, 'story': 0.14285714285714285}\n",
            "Document 2 Term Frequencies: {'own': 0.1111111111111111, 'house': 0.1111111111111111, 'decide': 0.1111111111111111, 'man': 0.1111111111111111, 'change': 0.1111111111111111, 'street': 0.2222222222222222, 'build': 0.1111111111111111, 'should': 0.1111111111111111}\n",
            "Document 3 Term Frequencies: {'be': 0.2857142857142857, 'house': 0.14285714285714285, 'happy': 0.14285714285714285, 'paint': 0.14285714285714285, 'neighbor': 0.14285714285714285, 'white': 0.14285714285714285}\n",
            "Document 4 Term Frequencies: {'red': 0.14285714285714285, 'wrong': 0.14285714285714285, 'be': 0.14285714285714285, 'think': 0.14285714285714285, 'house': 0.14285714285714285, 'man': 0.14285714285714285, 'paint': 0.14285714285714285}\n",
            "Document 5 Term Frequencies: {'explain': 0.2, 'neighbor': 0.2, 'house': 0.4, 'go': 0.2}\n",
            "Document 6 Term Frequencies: {'be': 0.125, 'think': 0.125, 'house': 0.125, 'idea': 0.125, 'paint': 0.125, 'neighbor': 0.125, 'orange': 0.125, 'good': 0.125}\n",
            "Document 7 Term Frequencies: {'think': 0.1111111111111111, 'house': 0.1111111111111111, 'look': 0.1111111111111111, 'rainbow': 0.1111111111111111, 'name': 0.1111111111111111, 'street': 0.1111111111111111, 'road': 0.1111111111111111, 'right': 0.1111111111111111, 'start': 0.1111111111111111}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inverse Document Frequency (IDF)\n",
        "\n",
        "The document frequency $df_t$ is the number of documents in which the term $t$ occurs. We again consider the relative document frequency, that is $df_t$ divided by the number of all documents $d$.\n",
        "\n",
        "$DF = \\frac{df_t}{d}$\n",
        "\n",
        "It has turned out that the inverse of this formula performs better, especially when scaling it with a logarithm. This gives us the Inverse Document Frequency (IDF):\n",
        "\n",
        "$IDF = \\log \\frac{d}{df_t}$\n",
        "\n",
        "ðŸ‘‹ âš’  Write a function to calculate the Inverse Document Frequency (IDF). The result will be a list of words with IDF values."
      ],
      "metadata": {
        "id": "1jX5HGqn94Zf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# the logarithm can be calculated with math.log()\n",
        "\n",
        "def compute_inverse_document_frequency(full_doc_list):\n",
        "    #frequency (df)\n",
        "    document_frequencies = {term: sum(1 for doc in full_doc_list if term in doc.split()) for term in set(' '.join(full_doc_list).split())}\n",
        "\n",
        "    #calculate the idf\n",
        "    idf_values = {term: math.log(len(full_doc_list) / df) for term, df in document_frequencies.items()}\n",
        "\n",
        "    return idf_values\n",
        "\n",
        "#compute IDF\n",
        "idf_values = compute_inverse_document_frequency(preprocessed)\n",
        "\n",
        "print(\"IDF Values: \", idf_values)\n"
      ],
      "metadata": {
        "id": "2WciKvu1GZgZ",
        "outputId": "b811aaa0-91de-4935-a7a3-f9012640166d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IDF Values:  {'own': 1.9459101490553132, 'be': 0.5596157879354227, 'think': 0.8472978603872037, 'man': 1.252762968495368, 'happy': 1.9459101490553132, 'paint': 0.8472978603872037, 'go': 1.9459101490553132, 'road': 1.9459101490553132, 'red': 1.252762968495368, 'house': 0.0, 'look': 1.9459101490553132, 'explain': 1.9459101490553132, 'idea': 1.9459101490553132, 'neighbor': 0.8472978603872037, 'build': 1.9459101490553132, 'should': 1.9459101490553132, 'white': 1.9459101490553132, 'rainbow': 1.9459101490553132, 'street': 0.8472978603872037, 'right': 1.9459101490553132, 'orange': 1.9459101490553132, 'good': 1.9459101490553132, 'start': 1.9459101490553132, 'wrong': 1.9459101490553132, 'twenty': 1.9459101490553132, 'decide': 1.9459101490553132, 'name': 1.9459101490553132, 'change': 1.9459101490553132, 'story': 1.9459101490553132}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF\n",
        "\n",
        "The final value is calculated by multiplying the values of the previous two calculations:\n",
        "\n",
        "$TF-IDF = tf_t \\ x \\ log \\frac{d}{df_t}$\n",
        "\n",
        "ðŸ‘‹ âš’  Write a function to calculate the final TF-IDF scores for each word."
      ],
      "metadata": {
        "id": "6qcAp5d1M73j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Your code should go here\n",
        "term_frequencies = [\n",
        "    {'red': 0.14285714285714285, 'be': 0.14285714285714285, 'house': 0.2857142857142857, 'twenty': 0.14285714285714285, 'street': 0.14285714285714285, 'story': 0.14285714285714285},\n",
        "    {'own': 0.1111111111111111, 'house': 0.1111111111111111, 'decide': 0.1111111111111111, 'man': 0.1111111111111111, 'change': 0.1111111111111111, 'street': 0.2222222222222222, 'build': 0.1111111111111111, 'should': 0.1111111111111111},\n",
        "    {'be': 0.2857142857142857, 'house': 0.14285714285714285, 'happy': 0.14285714285714285, 'paint': 0.14285714285714285, 'neighbor': 0.14285714285714285, 'white': 0.14285714285714285},\n",
        "    {'red': 0.14285714285714285, 'wrong': 0.14285714285714285, 'be': 0.14285714285714285, 'think': 0.14285714285714285, 'house': 0.14285714285714285, 'man': 0.14285714285714285, 'paint': 0.14285714285714285},\n",
        "    {'explain': 0.2, 'neighbor': 0.2, 'house': 0.4, 'go': 0.2},\n",
        "    {'be': 0.125, 'think': 0.125, 'house': 0.125, 'idea': 0.125, 'paint': 0.125, 'neighbor': 0.125, 'orange': 0.125, 'good': 0.125},\n",
        "    {'think': 0.1111111111111111, 'house': 0.1111111111111111, 'look': 0.1111111111111111, 'rainbow': 0.1111111111111111, 'name': 0.1111111111111111, 'street': 0.1111111111111111, 'road': 0.1111111111111111, 'right': 0.1111111111111111, 'start': 0.1111111111111111}\n",
        "]\n",
        "\n",
        "idf_values = {\n",
        "    'own': 1.9459101490553132, 'be': 0.5596157879354227, 'think': 0.8472978603872037,\n",
        "    'man': 1.252762968495368, 'happy': 1.9459101490553132, 'paint': 0.8472978603872037,\n",
        "    'go': 1.9459101490553132, 'road': 1.9459101490553132, 'red': 1.252762968495368,\n",
        "    'house': 0.0, 'look': 1.9459101490553132, 'explain': 1.9459101490553132,\n",
        "    'idea': 1.9459101490553132, 'neighbor': 0.8472978603872037, 'build': 1.9459101490553132,\n",
        "    'should': 1.9459101490553132, 'white': 1.9459101490553132, 'rainbow': 1.9459101490553132,\n",
        "    'street': 0.8472978603872037, 'right': 1.9459101490553132, 'orange': 1.9459101490553132,\n",
        "    'good': 1.9459101490553132, 'start': 1.9459101490553132, 'wrong': 1.9459101490553132,\n",
        "    'twenty': 1.9459101490553132, 'decide': 1.9459101490553132, 'name': 1.9459101490553132,\n",
        "    'change': 1.9459101490553132, 'story': 1.9459101490553132\n",
        "}\n",
        "\n",
        "#function to compute final TF-IDF\n",
        "def compute_final_tf_idf(tf_values, idf_values):\n",
        "    # Calculate the final TF-IDF for each term using the given formula\n",
        "    final_tf_idf_scores = {term: tf_values[term] * idf_values[term] for term in tf_values}\n",
        "    return final_tf_idf_scores\n",
        "\n",
        "# compute final TF-IDF scores\n",
        "example_final_tf_idf_scores = [compute_final_tf_idf(tf_values, idf_values) for tf_values in term_frequencies]\n",
        "\n",
        "for i, doc_final_tf_idf in enumerate(example_final_tf_idf_scores, 1):\n",
        "    print(f\"Document {i} Final TF-IDF Scores: {doc_final_tf_idf}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "VhsypMeDNU0c",
        "outputId": "13c49131-7566-4688-d357-05acbc7bccd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1 Final TF-IDF Scores: {'red': 0.17896613835648115, 'be': 0.07994511256220323, 'house': 0.0, 'twenty': 0.277987164150759, 'street': 0.12104255148388623, 'story': 0.277987164150759}\n",
            "Document 2 Final TF-IDF Scores: {'own': 0.21621223878392368, 'house': 0.0, 'decide': 0.21621223878392368, 'man': 0.1391958853883742, 'change': 0.21621223878392368, 'street': 0.18828841341937858, 'build': 0.21621223878392368, 'should': 0.21621223878392368}\n",
            "Document 3 Final TF-IDF Scores: {'be': 0.15989022512440645, 'house': 0.0, 'happy': 0.277987164150759, 'paint': 0.12104255148388623, 'neighbor': 0.12104255148388623, 'white': 0.277987164150759}\n",
            "Document 4 Final TF-IDF Scores: {'red': 0.17896613835648115, 'wrong': 0.277987164150759, 'be': 0.07994511256220323, 'think': 0.12104255148388623, 'house': 0.0, 'man': 0.17896613835648115, 'paint': 0.12104255148388623}\n",
            "Document 5 Final TF-IDF Scores: {'explain': 0.38918202981106265, 'neighbor': 0.16945957207744075, 'house': 0.0, 'go': 0.38918202981106265}\n",
            "Document 6 Final TF-IDF Scores: {'be': 0.06995197349192783, 'think': 0.10591223254840046, 'house': 0.0, 'idea': 0.24323876863191415, 'paint': 0.10591223254840046, 'neighbor': 0.10591223254840046, 'orange': 0.24323876863191415, 'good': 0.24323876863191415}\n",
            "Document 7 Final TF-IDF Scores: {'think': 0.09414420670968929, 'house': 0.0, 'look': 0.21621223878392368, 'rainbow': 0.21621223878392368, 'name': 0.21621223878392368, 'street': 0.09414420670968929, 'road': 0.21621223878392368, 'right': 0.21621223878392368, 'start': 0.21621223878392368}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare your results to sklearn\n",
        "\n",
        "sklearn provides an implementation for calculating the TF-IDF values. Compare your calculations to these values.\n",
        "\n",
        "ðŸ‘‹ âš’  Use the [TfidfVectorizer of sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) to calculate the TF-IDF values for the same corpus as above."
      ],
      "metadata": {
        "id": "E8z8KseAN9kh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#Your code here\n",
        "\n",
        "docs = [\n",
        "    \"this is the story behind the red house on the street with twenty houses\",\n",
        "    \"a man decided to build his own house on our street, which should change the street\",\n",
        "    \"all the houses were painted white and all the neighbors were happy with this\",\n",
        "    \"the man thought to himself this is wrong and painted his house red\",\n",
        "    \"he went from his house to his neighbor's house and explained\",\n",
        "    \"his neighbor thought this is a good idea and painted his house orange\",\n",
        "    \"now they thought the houses started to look right for a street named rainbow road\"\n",
        "]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "#fit and transform the corpus\n",
        "tfidf_sklearn = vectorizer.fit_transform(docs)\n",
        "\n",
        "#get terms\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "#dictionary\n",
        "tfidf_sklearn_dict = {term: tfidf_sklearn[:, vectorizer.vocabulary_[term]].toarray().flatten() for term in terms}\n",
        "\n",
        "for i, doc_tfidf_sklearn in enumerate(tfidf_sklearn_dict.values(), 1):\n",
        "    print(f\"Document {i} TF-IDF Scores (sklearn): {dict(zip(terms, doc_tfidf_sklearn))}\")"
      ],
      "metadata": {
        "id": "MMw5aTNcOF3C",
        "outputId": "68eec2f5-9b34-4089-fcb7-9d4defdc197b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.0, 'behind': 0.5230727856650398, 'build': 0.0, 'change': 0.0, 'decided': 0.0, 'explained': 0.0}\n",
            "Document 2 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.0, 'behind': 0.16111149274275427, 'build': 0.2330220550530006, 'change': 0.21603863218596028, 'decided': 0.2210326403843424, 'explained': 0.0}\n",
            "Document 3 TF-IDF Scores (sklearn): {'all': 0.31832354532454604, 'and': 0.0, 'behind': 0.0, 'build': 0.0, 'change': 0.0, 'decided': 0.0, 'explained': 0.0}\n",
            "Document 4 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.29193943488357654, 'behind': 0.0, 'build': 0.0, 'change': 0.0, 'decided': 0.0, 'explained': 0.0}\n",
            "Document 5 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.29193943488357654, 'behind': 0.0, 'build': 0.0, 'change': 0.0, 'decided': 0.0, 'explained': 0.0}\n",
            "Document 6 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.29193943488357654, 'behind': 0.0, 'build': 0.0, 'change': 0.0, 'decided': 0.0, 'explained': 0.0}\n",
            "Document 7 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.0, 'behind': 0.0, 'build': 0.0, 'change': 0.3507010183600244, 'decided': 0.0, 'explained': 0.0}\n",
            "Document 8 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.0, 'behind': 0.0, 'build': 0.0, 'change': 0.0, 'decided': 0.0, 'explained': 0.2990614213835297}\n",
            "Document 9 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.0, 'behind': 0.0, 'build': 0.0, 'change': 0.3507010183600244, 'decided': 0.0, 'explained': 0.0}\n",
            "Document 10 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.0, 'behind': 0.0, 'build': 0.0, 'change': 0.0, 'decided': 0.3588079191635961, 'explained': 0.0}\n",
            "Document 11 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.0, 'behind': 0.2615363928325199, 'build': 0.0, 'change': 0.0, 'decided': 0.0, 'explained': 0.0}\n",
            "Document 12 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.0, 'behind': 0.0, 'build': 0.0, 'change': 0.3507010183600244, 'decided': 0.0, 'explained': 0.0}\n",
            "Document 13 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.0, 'behind': 0.0, 'build': 0.3782706416002933, 'change': 0.0, 'decided': 0.0, 'explained': 0.0}\n",
            "Document 14 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.17984035657587738, 'behind': 0.0, 'build': 0.2330220550530006, 'change': 0.43207726437192057, 'decided': 0.4420652807686848, 'explained': 0.0}\n",
            "Document 15 TF-IDF Scores (sklearn): {'all': 0.17177240546356717, 'and': 0.15753512335538664, 'behind': 0.0, 'build': 0.2041208040633082, 'change': 0.37848760110286966, 'decided': 0.19361841208217875, 'explained': 0.0}\n",
            "Document 16 TF-IDF Scores (sklearn): {'all': 0.22586007076644246, 'and': 0.0, 'behind': 0.18556788858621098, 'build': 0.0, 'change': 0.0, 'decided': 0.0, 'explained': 0.21219301804499094}\n",
            "Document 17 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.0, 'behind': 0.0, 'build': 0.0, 'change': 0.0, 'decided': 0.3588079191635961, 'explained': 0.0}\n",
            "Document 18 TF-IDF Scores (sklearn): {'all': 0.22586007076644246, 'and': 0.0, 'behind': 0.0, 'build': 0.26839432751857395, 'change': 0.0, 'decided': 0.254584944168127, 'explained': 0.0}\n",
            "Document 19 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.0, 'behind': 0.0, 'build': 0.0, 'change': 0.0, 'decided': 0.0, 'explained': 0.2990614213835297}\n",
            "Document 20 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.2423348025068089, 'behind': 0.0, 'build': 0.31399711814502757, 'change': 0.0, 'decided': 0.0, 'explained': 0.0}\n",
            "Document 21 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.0, 'behind': 0.0, 'build': 0.0, 'change': 0.0, 'decided': 0.0, 'explained': 0.2990614213835297}\n",
            "Document 22 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.0, 'behind': 0.0, 'build': 0.0, 'change': 0.29111196319574134, 'decided': 0.29784138707764796, 'explained': 0.0}\n",
            "Document 23 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.0, 'behind': 0.2615363928325199, 'build': 0.0, 'change': 0.0, 'decided': 0.0, 'explained': 0.0}\n",
            "Document 24 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.0, 'behind': 0.0, 'build': 0.0, 'change': 0.0, 'decided': 0.0, 'explained': 0.2990614213835297}\n",
            "Document 25 TF-IDF Scores (sklearn): {'all': 0.26423588002167075, 'and': 0.2423348025068089, 'behind': 0.0, 'build': 0.0, 'change': 0.0, 'decided': 0.0, 'explained': 0.0}\n",
            "Document 26 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.0, 'behind': 0.0, 'build': 0.0, 'change': 0.0, 'decided': 0.3588079191635961, 'explained': 0.0}\n",
            "Document 27 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.29193943488357654, 'behind': 0.0, 'build': 0.0, 'change': 0.0, 'decided': 0.0, 'explained': 0.0}\n",
            "Document 28 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.29193943488357654, 'behind': 0.0, 'build': 0.0, 'change': 0.0, 'decided': 0.0, 'explained': 0.0}\n",
            "Document 29 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.0, 'behind': 0.18556788858621098, 'build': 0.26839432751857395, 'change': 0.0, 'decided': 0.254584944168127, 'explained': 0.0}\n",
            "Document 30 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.0, 'behind': 0.0, 'build': 0.0, 'change': 0.0, 'decided': 0.0, 'explained': 0.2990614213835297}\n",
            "Document 31 TF-IDF Scores (sklearn): {'all': 0.26423588002167075, 'and': 0.0, 'behind': 0.0, 'build': 0.31399711814502757, 'change': 0.0, 'decided': 0.0, 'explained': 0.0}\n",
            "Document 32 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.0, 'behind': 0.0, 'build': 0.0, 'change': 0.0, 'decided': 0.0, 'explained': 0.2990614213835297}\n",
            "Document 33 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.0, 'behind': 0.0, 'build': 0.0, 'change': 0.0, 'decided': 0.0, 'explained': 0.2990614213835297}\n",
            "Document 34 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.29193943488357654, 'behind': 0.0, 'build': 0.0, 'change': 0.0, 'decided': 0.0, 'explained': 0.0}\n",
            "Document 35 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.0, 'behind': 0.0, 'build': 0.0, 'change': 0.0, 'decided': 0.0, 'explained': 0.2990614213835297}\n",
            "Document 36 TF-IDF Scores (sklearn): {'all': 0.31832354532454604, 'and': 0.0, 'behind': 0.0, 'build': 0.0, 'change': 0.0, 'decided': 0.0, 'explained': 0.0}\n",
            "Document 37 TF-IDF Scores (sklearn): {'all': 0.22586007076644246, 'and': 0.41427951146430864, 'behind': 0.0, 'build': 0.0, 'change': 0.0, 'decided': 0.0, 'explained': 0.21219301804499094}\n",
            "Document 38 TF-IDF Scores (sklearn): {'all': 0.5153172163907015, 'and': 0.15753512335538664, 'behind': 0.28225832473248863, 'build': 0.2041208040633082, 'change': 0.0, 'decided': 0.0, 'explained': 0.16137825959442523}\n",
            "Document 39 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.0, 'behind': 0.0, 'build': 0.0, 'change': 0.0, 'decided': 0.0, 'explained': 0.2990614213835297}\n",
            "Document 40 TF-IDF Scores (sklearn): {'all': 0.19609348055528608, 'and': 0.0, 'behind': 0.16111149274275427, 'build': 0.2330220550530006, 'change': 0.0, 'decided': 0.2210326403843424, 'explained': 0.0}\n",
            "Document 41 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.0, 'behind': 0.0, 'build': 0.26839432751857395, 'change': 0.0, 'decided': 0.254584944168127, 'explained': 0.21219301804499094}\n",
            "Document 42 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.17984035657587738, 'behind': 0.0, 'build': 0.2330220550530006, 'change': 0.21603863218596028, 'decided': 0.0, 'explained': 0.18422763845230816}\n",
            "Document 43 TF-IDF Scores (sklearn): {'all': 0.31832354532454604, 'and': 0.0, 'behind': 0.0, 'build': 0.0, 'change': 0.0, 'decided': 0.0, 'explained': 0.0}\n",
            "Document 44 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.0, 'behind': 0.0, 'build': 0.0, 'change': 0.3507010183600244, 'decided': 0.0, 'explained': 0.0}\n",
            "Document 45 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.0, 'behind': 0.5230727856650398, 'build': 0.0, 'change': 0.0, 'decided': 0.0, 'explained': 0.0}\n",
            "Document 46 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.29193943488357654, 'behind': 0.0, 'build': 0.0, 'change': 0.0, 'decided': 0.0, 'explained': 0.0}\n",
            "Document 47 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.0, 'behind': 0.2615363928325199, 'build': 0.0, 'change': 0.0, 'decided': 0.0, 'explained': 0.0}\n",
            "Document 48 TF-IDF Scores (sklearn): {'all': 0.26423588002167075, 'and': 0.0, 'behind': 0.21709766661255325, 'build': 0.0, 'change': 0.0, 'decided': 0.0, 'explained': 0.0}\n",
            "Document 49 TF-IDF Scores (sklearn): {'all': 0.0, 'and': 0.0, 'behind': 0.0, 'build': 0.3782706416002933, 'change': 0.0, 'decided': 0.0, 'explained': 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What can you learn about the documents based on these extracted feautres?"
      ],
      "metadata": {
        "id": "LuDeyg_7SCfn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘‹ âš’  Write your textual answer right here.\n",
        "\n",
        "In exploring TF-IDF, it's like shining a light on words that really matter in our documents. When a term, say \"house,\" has a high TF-IDF, it means it's a big deal in that particular story. Consistent high values for words like \"rainbow\" and \"orange\" across documents give a unique touch. In Document 5, with \"explain\" and \"neighbor\" standing out, there's something interesting happening. On the other hand, common words like \"the\" with low TF-IDF just blend in. TF-IDF strikes a balance, highlighting important words while considering their rarity. Finding patterns, like the TF-IDF 0.0 for \"house\" in Document 3, adds an interesting layer. Despite academic challenges, working with TF-IDF gives a practical insight into our textual data."
      ],
      "metadata": {
        "id": "Jmu6mLsGCoge"
      }
    }
  ]
}